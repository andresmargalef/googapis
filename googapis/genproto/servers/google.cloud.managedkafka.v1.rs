// This file is @generated by prost-build.
/// An Apache Kafka cluster deployed in a location.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Cluster {
    /// Identifier. The name of the cluster. Structured like:
    /// projects/{project_number}/locations/{location}/clusters/{cluster_id}
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Output only. The time when the cluster was created.
    #[prost(message, optional, tag = "2")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. The time when the cluster was last updated.
    #[prost(message, optional, tag = "3")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Optional. Labels as key value pairs.
    #[prost(map = "string, string", tag = "4")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Required. Capacity configuration for the Kafka cluster.
    #[prost(message, optional, tag = "5")]
    pub capacity_config: ::core::option::Option<CapacityConfig>,
    /// Optional. Rebalance configuration for the Kafka cluster.
    #[prost(message, optional, tag = "8")]
    pub rebalance_config: ::core::option::Option<RebalanceConfig>,
    /// Output only. The current state of the cluster.
    #[prost(enumeration = "cluster::State", tag = "10")]
    pub state: i32,
    /// Platform specific configuration properties for a Kafka cluster.
    #[prost(oneof = "cluster::PlatformConfig", tags = "9")]
    pub platform_config: ::core::option::Option<cluster::PlatformConfig>,
}
/// Nested message and enum types in `Cluster`.
pub mod cluster {
    /// The state of the cluster.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum State {
        /// A state was not specified.
        Unspecified = 0,
        /// The cluster is being created.
        Creating = 1,
        /// The cluster is active.
        Active = 2,
        /// The cluster is being deleted.
        Deleting = 3,
    }
    impl State {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "STATE_UNSPECIFIED",
                Self::Creating => "CREATING",
                Self::Active => "ACTIVE",
                Self::Deleting => "DELETING",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "STATE_UNSPECIFIED" => Some(Self::Unspecified),
                "CREATING" => Some(Self::Creating),
                "ACTIVE" => Some(Self::Active),
                "DELETING" => Some(Self::Deleting),
                _ => None,
            }
        }
    }
    /// Platform specific configuration properties for a Kafka cluster.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum PlatformConfig {
        /// Required. Configuration properties for a Kafka cluster deployed to Google
        /// Cloud Platform.
        #[prost(message, tag = "9")]
        GcpConfig(super::GcpConfig),
    }
}
/// A capacity configuration of a Kafka cluster.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct CapacityConfig {
    /// Required. The number of vCPUs to provision for the cluster. Minimum: 3.
    #[prost(int64, tag = "1")]
    pub vcpu_count: i64,
    /// Required. The memory to provision for the cluster in bytes.
    /// The CPU:memory ratio (vCPU:GiB) must be between 1:1 and 1:8.
    /// Minimum: 3221225472 (3 GiB).
    #[prost(int64, tag = "2")]
    pub memory_bytes: i64,
}
/// Defines rebalancing behavior of a Kafka cluster.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct RebalanceConfig {
    /// Optional. The rebalance behavior for the cluster.
    /// When not specified, defaults to `NO_REBALANCE`.
    #[prost(enumeration = "rebalance_config::Mode", tag = "1")]
    pub mode: i32,
}
/// Nested message and enum types in `RebalanceConfig`.
pub mod rebalance_config {
    /// The partition rebalance mode for the cluster.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum Mode {
        /// A mode was not specified. Do not use.
        Unspecified = 0,
        /// Do not rebalance automatically.
        NoRebalance = 1,
        /// Automatically rebalance topic partitions among brokers when the
        /// cluster is scaled up.
        AutoRebalanceOnScaleUp = 2,
    }
    impl Mode {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "MODE_UNSPECIFIED",
                Self::NoRebalance => "NO_REBALANCE",
                Self::AutoRebalanceOnScaleUp => "AUTO_REBALANCE_ON_SCALE_UP",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "MODE_UNSPECIFIED" => Some(Self::Unspecified),
                "NO_REBALANCE" => Some(Self::NoRebalance),
                "AUTO_REBALANCE_ON_SCALE_UP" => Some(Self::AutoRebalanceOnScaleUp),
                _ => None,
            }
        }
    }
}
/// The configuration of a Virtual Private Cloud (VPC) network that can access
/// the Kafka cluster.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct NetworkConfig {
    /// Required. Name of the VPC subnet in which to create Private Service Connect
    /// (PSC) endpoints for the Kafka brokers and bootstrap address. Structured
    /// like: projects/{project}/regions/{region}/subnetworks/{subnet_id}
    ///
    /// The subnet must be located in the same region as the Kafka cluster. The
    /// project may differ. Multiple subnets from the same parent network must not
    /// be specified.
    ///
    /// The CIDR range of the subnet must be within the IPv4 address ranges for
    /// private networks, as specified in RFC 1918.
    #[prost(string, tag = "2")]
    pub subnet: ::prost::alloc::string::String,
}
/// The configuration of access to the Kafka cluster.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct AccessConfig {
    /// Required. Virtual Private Cloud (VPC) networks that must be granted direct
    /// access to the Kafka cluster. Minimum of 1 network is required. Maximum 10
    /// networks can be specified.
    #[prost(message, repeated, tag = "1")]
    pub network_configs: ::prost::alloc::vec::Vec<NetworkConfig>,
}
/// Configuration properties for a Kafka cluster deployed to Google Cloud
/// Platform.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GcpConfig {
    /// Required. Access configuration for the Kafka cluster.
    #[prost(message, optional, tag = "3")]
    pub access_config: ::core::option::Option<AccessConfig>,
    /// Optional. Immutable. The Cloud KMS Key name to use for encryption. The key
    /// must be located in the same region as the cluster and cannot be changed.
    /// Structured like:
    /// projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}.
    /// Note that the project component only accepts a project ID, and not a
    /// project number.
    #[prost(string, tag = "2")]
    pub kms_key: ::prost::alloc::string::String,
}
/// A Kafka topic in a given cluster.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Topic {
    /// Identifier. The name of the topic. The `topic` segment is used when
    /// connecting directly to the cluster. Structured like:
    /// projects/{project}/locations/{location}/clusters/{cluster}/topics/{topic}
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The number of partitions this topic has. The partition count can
    /// only be increased, not decreased. Please note that if partitions are
    /// increased for a topic that has a key, the partitioning logic or the
    /// ordering of the messages will be affected.
    #[prost(int32, tag = "2")]
    pub partition_count: i32,
    /// Required. Immutable. The number of replicas of each partition. A
    /// replication factor of 3 is recommended for high availability.
    #[prost(int32, tag = "3")]
    pub replication_factor: i32,
    /// Optional. Configurations for the topic that are overridden from the cluster
    /// defaults. The key of the map is a Kafka topic property name, for example:
    /// `cleanup.policy`, `compression.type`.
    #[prost(map = "string, string", tag = "4")]
    pub configs: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
}
/// Metadata for a consumer group corresponding to a specific topic.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ConsumerTopicMetadata {
    /// Optional. Metadata for this consumer group and topic for all partition
    /// indexes it has metadata for.
    #[prost(map = "int32, message", tag = "1")]
    pub partitions: ::std::collections::HashMap<i32, ConsumerPartitionMetadata>,
}
/// Metadata for a consumer group corresponding to a specific partition.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ConsumerPartitionMetadata {
    /// Required. The offset for this partition, or 0 if no offset has been
    /// committed.
    #[prost(int64, tag = "1")]
    pub offset: i64,
    /// Optional. The associated metadata for this partition, or empty if it does
    /// not exist.
    #[prost(string, tag = "2")]
    pub metadata: ::prost::alloc::string::String,
}
/// A Kafka consumer group in a given cluster.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ConsumerGroup {
    /// Identifier. The name of the consumer group. The `consumer_group` segment is
    /// used when connecting directly to the cluster. Structured like:
    /// projects/{project}/locations/{location}/clusters/{cluster}/consumerGroups/{consumer_group}
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. Metadata for this consumer group for all topics it has metadata
    /// for. The key of the map is a topic name, structured like:
    /// projects/{project}/locations/{location}/clusters/{cluster}/topics/{topic}
    #[prost(map = "string, message", tag = "2")]
    pub topics: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ConsumerTopicMetadata,
    >,
}
/// Represents the metadata of the long-running operation.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct OperationMetadata {
    /// Output only. The time the operation was created.
    #[prost(message, optional, tag = "1")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. The time the operation finished running.
    #[prost(message, optional, tag = "2")]
    pub end_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Server-defined resource path for the target of the operation.
    #[prost(string, tag = "3")]
    pub target: ::prost::alloc::string::String,
    /// Output only. Name of the verb executed by the operation.
    #[prost(string, tag = "4")]
    pub verb: ::prost::alloc::string::String,
    /// Output only. Human-readable status of the operation, if any.
    #[prost(string, tag = "5")]
    pub status_message: ::prost::alloc::string::String,
    /// Output only. Identifies whether the user has requested cancellation
    /// of the operation. Operations that have been cancelled successfully
    /// have [Operation.error][] value with a
    /// [google.rpc.Status.code][google.rpc.Status.code] of 1, corresponding to
    /// `Code.CANCELLED`.
    #[prost(bool, tag = "6")]
    pub requested_cancellation: bool,
    /// Output only. API version used to start the operation.
    #[prost(string, tag = "7")]
    pub api_version: ::prost::alloc::string::String,
}
/// Request for ListClusters.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListClustersRequest {
    /// Required. The parent location whose clusters are to be listed. Structured
    /// like `projects/{project}/locations/{location}`.
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Optional. The maximum number of clusters to return. The service may return
    /// fewer than this value. If unspecified, server will pick an appropriate
    /// default.
    #[prost(int32, tag = "2")]
    pub page_size: i32,
    /// Optional. A page token, received from a previous `ListClusters` call.
    /// Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other parameters provided to `ListClusters` must match
    /// the call that provided the page token.
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
    /// Optional. Filter expression for the result.
    #[prost(string, tag = "4")]
    pub filter: ::prost::alloc::string::String,
    /// Optional. Order by fields for the result.
    #[prost(string, tag = "5")]
    pub order_by: ::prost::alloc::string::String,
}
/// Response for ListClusters.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListClustersResponse {
    /// The list of Clusters in the requested parent.
    #[prost(message, repeated, tag = "1")]
    pub clusters: ::prost::alloc::vec::Vec<Cluster>,
    /// A token that can be sent as `page_token` to retrieve the next page of
    /// results. If this field is omitted, there are no more results.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
    /// Locations that could not be reached.
    #[prost(string, repeated, tag = "3")]
    pub unreachable: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Request for GetCluster.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetClusterRequest {
    /// Required. The name of the cluster whose configuration to return.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request for CreateCluster.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateClusterRequest {
    /// Required. The parent region in which to create the cluster. Structured like
    /// `projects/{project}/locations/{location}`.
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The ID to use for the cluster, which will become the final
    /// component of the cluster's name. The ID must be 1-63 characters long, and
    /// match the regular expression `[a-z](\[-a-z0-9\]*[a-z0-9])?` to comply with
    /// RFC 1035.
    ///
    /// This value is structured like: `my-cluster-id`.
    #[prost(string, tag = "2")]
    pub cluster_id: ::prost::alloc::string::String,
    /// Required. Configuration of the cluster to create. Its `name` field is
    /// ignored.
    #[prost(message, optional, tag = "3")]
    pub cluster: ::core::option::Option<Cluster>,
    /// Optional. An optional request ID to identify requests. Specify a unique
    /// request ID to avoid duplication of requests. If a request times out or
    /// fails, retrying with the same ID allows the server to recognize the
    /// previous attempt. For at least 60 minutes, the server ignores duplicate
    /// requests bearing the same ID.
    ///
    /// For example, consider a situation where you make an initial request and the
    /// request times out. If you make the request again with the same request ID
    /// within 60 minutes of the last request, the server checks if an original
    /// operation with the same request ID was received. If so, the server ignores
    /// the second request.
    ///
    /// The request ID must be a valid UUID. A zero UUID is not supported
    /// (00000000-0000-0000-0000-000000000000).
    #[prost(string, tag = "4")]
    pub request_id: ::prost::alloc::string::String,
}
/// Request for UpdateCluster.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateClusterRequest {
    /// Required. Field mask is used to specify the fields to be overwritten in the
    /// cluster resource by the update. The fields specified in the update_mask are
    /// relative to the resource, not the full request. A field will be overwritten
    /// if it is in the mask. The mask is required and a value of * will update all
    /// fields.
    #[prost(message, optional, tag = "1")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// Required. The cluster to update. Its `name` field must be populated.
    #[prost(message, optional, tag = "2")]
    pub cluster: ::core::option::Option<Cluster>,
    /// Optional. An optional request ID to identify requests. Specify a unique
    /// request ID to avoid duplication of requests. If a request times out or
    /// fails, retrying with the same ID allows the server to recognize the
    /// previous attempt. For at least 60 minutes, the server ignores duplicate
    /// requests bearing the same ID.
    ///
    /// For example, consider a situation where you make an initial request and the
    /// request times out. If you make the request again with the same request ID
    /// within 60 minutes of the last request, the server checks if an original
    /// operation with the same request ID was received. If so, the server ignores
    /// the second request.
    ///
    /// The request ID must be a valid UUID. A zero UUID is not supported
    /// (00000000-0000-0000-0000-000000000000).
    #[prost(string, tag = "3")]
    pub request_id: ::prost::alloc::string::String,
}
/// Request for DeleteCluster.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteClusterRequest {
    /// Required. The name of the cluster to delete.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. An optional request ID to identify requests. Specify a unique
    /// request ID to avoid duplication of requests. If a request times out or
    /// fails, retrying with the same ID allows the server to recognize the
    /// previous attempt. For at least 60 minutes, the server ignores duplicate
    /// requests bearing the same ID.
    ///
    /// For example, consider a situation where you make an initial request and the
    /// request times out. If you make the request again with the same request ID
    /// within 60 minutes of the last request, the server checks if an original
    /// operation with the same request ID was received. If so, the server ignores
    /// the second request.
    ///
    /// The request ID must be a valid UUID. A zero UUID is not supported
    /// (00000000-0000-0000-0000-000000000000).
    #[prost(string, tag = "2")]
    pub request_id: ::prost::alloc::string::String,
}
/// Request for ListTopics.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListTopicsRequest {
    /// Required. The parent cluster whose topics are to be listed. Structured like
    /// `projects/{project}/locations/{location}/clusters/{cluster}`.
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Optional. The maximum number of topics to return. The service may return
    /// fewer than this value. If unset or zero, all topics for the parent is
    /// returned.
    #[prost(int32, tag = "2")]
    pub page_size: i32,
    /// Optional. A page token, received from a previous `ListTopics` call.
    /// Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other parameters provided to `ListTopics` must match
    /// the call that provided the page token.
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
}
/// Response for ListTopics.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListTopicsResponse {
    /// The list of topics in the requested parent. The order of the topics is
    /// unspecified.
    #[prost(message, repeated, tag = "1")]
    pub topics: ::prost::alloc::vec::Vec<Topic>,
    /// A token that can be sent as `page_token` to retrieve the next page of
    /// results. If this field is omitted, there are no more results.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request for GetTopic.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetTopicRequest {
    /// Required. The name of the topic whose configuration to return. Structured
    /// like:
    /// projects/{project}/locations/{location}/clusters/{cluster}/topics/{topic}.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request for CreateTopic.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateTopicRequest {
    /// Required. The parent cluster in which to create the topic.
    /// Structured like
    /// `projects/{project}/locations/{location}/clusters/{cluster}`.
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The ID to use for the topic, which will become the final
    /// component of the topic's name.
    ///
    /// This value is structured like: `my-topic-name`.
    #[prost(string, tag = "2")]
    pub topic_id: ::prost::alloc::string::String,
    /// Required. Configuration of the topic to create. Its `name` field is
    /// ignored.
    #[prost(message, optional, tag = "3")]
    pub topic: ::core::option::Option<Topic>,
}
/// Request for UpdateTopic.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateTopicRequest {
    /// Required. Field mask is used to specify the fields to be overwritten in the
    /// Topic resource by the update. The fields specified in the update_mask are
    /// relative to the resource, not the full request. A field will be overwritten
    /// if it is in the mask. The mask is required and a value of * will update all
    /// fields.
    #[prost(message, optional, tag = "1")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// Required. The topic to update. Its `name` field must be populated.
    #[prost(message, optional, tag = "2")]
    pub topic: ::core::option::Option<Topic>,
}
/// Request for DeleteTopic.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteTopicRequest {
    /// Required. The name of the topic to delete.
    /// `projects/{project}/locations/{location}/clusters/{cluster}/topics/{topic}`.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request for ListConsumerGroups.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListConsumerGroupsRequest {
    /// Required. The parent cluster whose consumer groups are to be listed.
    /// Structured like
    /// `projects/{project}/locations/{location}/clusters/{cluster}`.
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Optional. The maximum number of consumer groups to return. The service may
    /// return fewer than this value. If unset or zero, all consumer groups for the
    /// parent is returned.
    #[prost(int32, tag = "2")]
    pub page_size: i32,
    /// Optional. A page token, received from a previous `ListConsumerGroups` call.
    /// Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other parameters provided to `ListConsumerGroups` must
    /// match the call that provided the page token.
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
}
/// Response for ListConsumerGroups.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListConsumerGroupsResponse {
    /// The list of consumer group in the requested parent. The order of the
    /// consumer groups is unspecified.
    #[prost(message, repeated, tag = "1")]
    pub consumer_groups: ::prost::alloc::vec::Vec<ConsumerGroup>,
    /// A token that can be sent as `page_token` to retrieve the next page of
    /// results. If this field is omitted, there are no more results.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request for GetConsumerGroup.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetConsumerGroupRequest {
    /// Required. The name of the consumer group whose configuration to return.
    /// `projects/{project}/locations/{location}/clusters/{cluster}/consumerGroups/{consumerGroup}`.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request for UpdateConsumerGroup.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateConsumerGroupRequest {
    /// Required. Field mask is used to specify the fields to be overwritten in the
    /// ConsumerGroup resource by the update.
    /// The fields specified in the update_mask are relative to the resource, not
    /// the full request. A field will be overwritten if it is in the mask. The
    /// mask is required and a value of * will update all fields.
    #[prost(message, optional, tag = "1")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// Required. The consumer group to update. Its `name` field must be populated.
    #[prost(message, optional, tag = "2")]
    pub consumer_group: ::core::option::Option<ConsumerGroup>,
}
/// Request for DeleteConsumerGroup.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteConsumerGroupRequest {
    /// Required. The name of the consumer group to delete.
    /// `projects/{project}/locations/{location}/clusters/{cluster}/consumerGroups/{consumerGroup}`.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Generated server implementations.
pub mod managed_kafka_server {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    /// Generated trait containing gRPC methods that should be implemented for use with ManagedKafkaServer.
    #[async_trait]
    pub trait ManagedKafka: std::marker::Send + std::marker::Sync + 'static {
        /// Lists the clusters in a given project and location.
        async fn list_clusters(
            &self,
            request: tonic::Request<super::ListClustersRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListClustersResponse>,
            tonic::Status,
        >;
        /// Returns the properties of a single cluster.
        async fn get_cluster(
            &self,
            request: tonic::Request<super::GetClusterRequest>,
        ) -> std::result::Result<tonic::Response<super::Cluster>, tonic::Status>;
        /// Creates a new cluster in a given project and location.
        async fn create_cluster(
            &self,
            request: tonic::Request<super::CreateClusterRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        >;
        /// Updates the properties of a single cluster.
        async fn update_cluster(
            &self,
            request: tonic::Request<super::UpdateClusterRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        >;
        /// Deletes a single cluster.
        async fn delete_cluster(
            &self,
            request: tonic::Request<super::DeleteClusterRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        >;
        /// Lists the topics in a given cluster.
        async fn list_topics(
            &self,
            request: tonic::Request<super::ListTopicsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListTopicsResponse>,
            tonic::Status,
        >;
        /// Returns the properties of a single topic.
        async fn get_topic(
            &self,
            request: tonic::Request<super::GetTopicRequest>,
        ) -> std::result::Result<tonic::Response<super::Topic>, tonic::Status>;
        /// Creates a new topic in a given project and location.
        async fn create_topic(
            &self,
            request: tonic::Request<super::CreateTopicRequest>,
        ) -> std::result::Result<tonic::Response<super::Topic>, tonic::Status>;
        /// Updates the properties of a single topic.
        async fn update_topic(
            &self,
            request: tonic::Request<super::UpdateTopicRequest>,
        ) -> std::result::Result<tonic::Response<super::Topic>, tonic::Status>;
        /// Deletes a single topic.
        async fn delete_topic(
            &self,
            request: tonic::Request<super::DeleteTopicRequest>,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status>;
        /// Lists the consumer groups in a given cluster.
        async fn list_consumer_groups(
            &self,
            request: tonic::Request<super::ListConsumerGroupsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListConsumerGroupsResponse>,
            tonic::Status,
        >;
        /// Returns the properties of a single consumer group.
        async fn get_consumer_group(
            &self,
            request: tonic::Request<super::GetConsumerGroupRequest>,
        ) -> std::result::Result<tonic::Response<super::ConsumerGroup>, tonic::Status>;
        /// Updates the properties of a single consumer group.
        async fn update_consumer_group(
            &self,
            request: tonic::Request<super::UpdateConsumerGroupRequest>,
        ) -> std::result::Result<tonic::Response<super::ConsumerGroup>, tonic::Status>;
        /// Deletes a single consumer group.
        async fn delete_consumer_group(
            &self,
            request: tonic::Request<super::DeleteConsumerGroupRequest>,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status>;
    }
    /// The service that a client application uses to manage Apache Kafka clusters,
    /// topics and consumer groups.
    #[derive(Debug)]
    pub struct ManagedKafkaServer<T> {
        inner: Arc<T>,
        accept_compression_encodings: EnabledCompressionEncodings,
        send_compression_encodings: EnabledCompressionEncodings,
        max_decoding_message_size: Option<usize>,
        max_encoding_message_size: Option<usize>,
    }
    impl<T> ManagedKafkaServer<T> {
        pub fn new(inner: T) -> Self {
            Self::from_arc(Arc::new(inner))
        }
        pub fn from_arc(inner: Arc<T>) -> Self {
            Self {
                inner,
                accept_compression_encodings: Default::default(),
                send_compression_encodings: Default::default(),
                max_decoding_message_size: None,
                max_encoding_message_size: None,
            }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> InterceptedService<Self, F>
        where
            F: tonic::service::Interceptor,
        {
            InterceptedService::new(Self::new(inner), interceptor)
        }
        /// Enable decompressing requests with the given encoding.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.accept_compression_encodings.enable(encoding);
            self
        }
        /// Compress responses with the given encoding, if the client supports it.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.send_compression_encodings.enable(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.max_decoding_message_size = Some(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.max_encoding_message_size = Some(limit);
            self
        }
    }
    impl<T, B> tonic::codegen::Service<http::Request<B>> for ManagedKafkaServer<T>
    where
        T: ManagedKafka,
        B: Body + std::marker::Send + 'static,
        B::Error: Into<StdError> + std::marker::Send + 'static,
    {
        type Response = http::Response<tonic::body::BoxBody>;
        type Error = std::convert::Infallible;
        type Future = BoxFuture<Self::Response, Self::Error>;
        fn poll_ready(
            &mut self,
            _cx: &mut Context<'_>,
        ) -> Poll<std::result::Result<(), Self::Error>> {
            Poll::Ready(Ok(()))
        }
        fn call(&mut self, req: http::Request<B>) -> Self::Future {
            match req.uri().path() {
                "/google.cloud.managedkafka.v1.ManagedKafka/ListClusters" => {
                    #[allow(non_camel_case_types)]
                    struct ListClustersSvc<T: ManagedKafka>(pub Arc<T>);
                    impl<
                        T: ManagedKafka,
                    > tonic::server::UnaryService<super::ListClustersRequest>
                    for ListClustersSvc<T> {
                        type Response = super::ListClustersResponse;
                        type Future = BoxFuture<
                            tonic::Response<Self::Response>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<super::ListClustersRequest>,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as ManagedKafka>::list_clusters(&inner, request).await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = ListClustersSvc(inner);
                        let codec = tonic::codec::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.unary(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                "/google.cloud.managedkafka.v1.ManagedKafka/GetCluster" => {
                    #[allow(non_camel_case_types)]
                    struct GetClusterSvc<T: ManagedKafka>(pub Arc<T>);
                    impl<
                        T: ManagedKafka,
                    > tonic::server::UnaryService<super::GetClusterRequest>
                    for GetClusterSvc<T> {
                        type Response = super::Cluster;
                        type Future = BoxFuture<
                            tonic::Response<Self::Response>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<super::GetClusterRequest>,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as ManagedKafka>::get_cluster(&inner, request).await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = GetClusterSvc(inner);
                        let codec = tonic::codec::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.unary(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                "/google.cloud.managedkafka.v1.ManagedKafka/CreateCluster" => {
                    #[allow(non_camel_case_types)]
                    struct CreateClusterSvc<T: ManagedKafka>(pub Arc<T>);
                    impl<
                        T: ManagedKafka,
                    > tonic::server::UnaryService<super::CreateClusterRequest>
                    for CreateClusterSvc<T> {
                        type Response = super::super::super::super::longrunning::Operation;
                        type Future = BoxFuture<
                            tonic::Response<Self::Response>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<super::CreateClusterRequest>,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as ManagedKafka>::create_cluster(&inner, request).await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = CreateClusterSvc(inner);
                        let codec = tonic::codec::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.unary(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                "/google.cloud.managedkafka.v1.ManagedKafka/UpdateCluster" => {
                    #[allow(non_camel_case_types)]
                    struct UpdateClusterSvc<T: ManagedKafka>(pub Arc<T>);
                    impl<
                        T: ManagedKafka,
                    > tonic::server::UnaryService<super::UpdateClusterRequest>
                    for UpdateClusterSvc<T> {
                        type Response = super::super::super::super::longrunning::Operation;
                        type Future = BoxFuture<
                            tonic::Response<Self::Response>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<super::UpdateClusterRequest>,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as ManagedKafka>::update_cluster(&inner, request).await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = UpdateClusterSvc(inner);
                        let codec = tonic::codec::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.unary(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                "/google.cloud.managedkafka.v1.ManagedKafka/DeleteCluster" => {
                    #[allow(non_camel_case_types)]
                    struct DeleteClusterSvc<T: ManagedKafka>(pub Arc<T>);
                    impl<
                        T: ManagedKafka,
                    > tonic::server::UnaryService<super::DeleteClusterRequest>
                    for DeleteClusterSvc<T> {
                        type Response = super::super::super::super::longrunning::Operation;
                        type Future = BoxFuture<
                            tonic::Response<Self::Response>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<super::DeleteClusterRequest>,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as ManagedKafka>::delete_cluster(&inner, request).await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = DeleteClusterSvc(inner);
                        let codec = tonic::codec::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.unary(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                "/google.cloud.managedkafka.v1.ManagedKafka/ListTopics" => {
                    #[allow(non_camel_case_types)]
                    struct ListTopicsSvc<T: ManagedKafka>(pub Arc<T>);
                    impl<
                        T: ManagedKafka,
                    > tonic::server::UnaryService<super::ListTopicsRequest>
                    for ListTopicsSvc<T> {
                        type Response = super::ListTopicsResponse;
                        type Future = BoxFuture<
                            tonic::Response<Self::Response>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<super::ListTopicsRequest>,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as ManagedKafka>::list_topics(&inner, request).await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = ListTopicsSvc(inner);
                        let codec = tonic::codec::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.unary(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                "/google.cloud.managedkafka.v1.ManagedKafka/GetTopic" => {
                    #[allow(non_camel_case_types)]
                    struct GetTopicSvc<T: ManagedKafka>(pub Arc<T>);
                    impl<
                        T: ManagedKafka,
                    > tonic::server::UnaryService<super::GetTopicRequest>
                    for GetTopicSvc<T> {
                        type Response = super::Topic;
                        type Future = BoxFuture<
                            tonic::Response<Self::Response>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<super::GetTopicRequest>,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as ManagedKafka>::get_topic(&inner, request).await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = GetTopicSvc(inner);
                        let codec = tonic::codec::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.unary(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                "/google.cloud.managedkafka.v1.ManagedKafka/CreateTopic" => {
                    #[allow(non_camel_case_types)]
                    struct CreateTopicSvc<T: ManagedKafka>(pub Arc<T>);
                    impl<
                        T: ManagedKafka,
                    > tonic::server::UnaryService<super::CreateTopicRequest>
                    for CreateTopicSvc<T> {
                        type Response = super::Topic;
                        type Future = BoxFuture<
                            tonic::Response<Self::Response>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<super::CreateTopicRequest>,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as ManagedKafka>::create_topic(&inner, request).await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = CreateTopicSvc(inner);
                        let codec = tonic::codec::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.unary(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                "/google.cloud.managedkafka.v1.ManagedKafka/UpdateTopic" => {
                    #[allow(non_camel_case_types)]
                    struct UpdateTopicSvc<T: ManagedKafka>(pub Arc<T>);
                    impl<
                        T: ManagedKafka,
                    > tonic::server::UnaryService<super::UpdateTopicRequest>
                    for UpdateTopicSvc<T> {
                        type Response = super::Topic;
                        type Future = BoxFuture<
                            tonic::Response<Self::Response>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<super::UpdateTopicRequest>,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as ManagedKafka>::update_topic(&inner, request).await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = UpdateTopicSvc(inner);
                        let codec = tonic::codec::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.unary(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                "/google.cloud.managedkafka.v1.ManagedKafka/DeleteTopic" => {
                    #[allow(non_camel_case_types)]
                    struct DeleteTopicSvc<T: ManagedKafka>(pub Arc<T>);
                    impl<
                        T: ManagedKafka,
                    > tonic::server::UnaryService<super::DeleteTopicRequest>
                    for DeleteTopicSvc<T> {
                        type Response = ();
                        type Future = BoxFuture<
                            tonic::Response<Self::Response>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<super::DeleteTopicRequest>,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as ManagedKafka>::delete_topic(&inner, request).await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = DeleteTopicSvc(inner);
                        let codec = tonic::codec::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.unary(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                "/google.cloud.managedkafka.v1.ManagedKafka/ListConsumerGroups" => {
                    #[allow(non_camel_case_types)]
                    struct ListConsumerGroupsSvc<T: ManagedKafka>(pub Arc<T>);
                    impl<
                        T: ManagedKafka,
                    > tonic::server::UnaryService<super::ListConsumerGroupsRequest>
                    for ListConsumerGroupsSvc<T> {
                        type Response = super::ListConsumerGroupsResponse;
                        type Future = BoxFuture<
                            tonic::Response<Self::Response>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<super::ListConsumerGroupsRequest>,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as ManagedKafka>::list_consumer_groups(&inner, request)
                                    .await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = ListConsumerGroupsSvc(inner);
                        let codec = tonic::codec::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.unary(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                "/google.cloud.managedkafka.v1.ManagedKafka/GetConsumerGroup" => {
                    #[allow(non_camel_case_types)]
                    struct GetConsumerGroupSvc<T: ManagedKafka>(pub Arc<T>);
                    impl<
                        T: ManagedKafka,
                    > tonic::server::UnaryService<super::GetConsumerGroupRequest>
                    for GetConsumerGroupSvc<T> {
                        type Response = super::ConsumerGroup;
                        type Future = BoxFuture<
                            tonic::Response<Self::Response>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<super::GetConsumerGroupRequest>,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as ManagedKafka>::get_consumer_group(&inner, request)
                                    .await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = GetConsumerGroupSvc(inner);
                        let codec = tonic::codec::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.unary(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                "/google.cloud.managedkafka.v1.ManagedKafka/UpdateConsumerGroup" => {
                    #[allow(non_camel_case_types)]
                    struct UpdateConsumerGroupSvc<T: ManagedKafka>(pub Arc<T>);
                    impl<
                        T: ManagedKafka,
                    > tonic::server::UnaryService<super::UpdateConsumerGroupRequest>
                    for UpdateConsumerGroupSvc<T> {
                        type Response = super::ConsumerGroup;
                        type Future = BoxFuture<
                            tonic::Response<Self::Response>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<super::UpdateConsumerGroupRequest>,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as ManagedKafka>::update_consumer_group(&inner, request)
                                    .await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = UpdateConsumerGroupSvc(inner);
                        let codec = tonic::codec::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.unary(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                "/google.cloud.managedkafka.v1.ManagedKafka/DeleteConsumerGroup" => {
                    #[allow(non_camel_case_types)]
                    struct DeleteConsumerGroupSvc<T: ManagedKafka>(pub Arc<T>);
                    impl<
                        T: ManagedKafka,
                    > tonic::server::UnaryService<super::DeleteConsumerGroupRequest>
                    for DeleteConsumerGroupSvc<T> {
                        type Response = ();
                        type Future = BoxFuture<
                            tonic::Response<Self::Response>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<super::DeleteConsumerGroupRequest>,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as ManagedKafka>::delete_consumer_group(&inner, request)
                                    .await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = DeleteConsumerGroupSvc(inner);
                        let codec = tonic::codec::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.unary(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                _ => {
                    Box::pin(async move {
                        let mut response = http::Response::new(empty_body());
                        let headers = response.headers_mut();
                        headers
                            .insert(
                                tonic::Status::GRPC_STATUS,
                                (tonic::Code::Unimplemented as i32).into(),
                            );
                        headers
                            .insert(
                                http::header::CONTENT_TYPE,
                                tonic::metadata::GRPC_CONTENT_TYPE,
                            );
                        Ok(response)
                    })
                }
            }
        }
    }
    impl<T> Clone for ManagedKafkaServer<T> {
        fn clone(&self) -> Self {
            let inner = self.inner.clone();
            Self {
                inner,
                accept_compression_encodings: self.accept_compression_encodings,
                send_compression_encodings: self.send_compression_encodings,
                max_decoding_message_size: self.max_decoding_message_size,
                max_encoding_message_size: self.max_encoding_message_size,
            }
        }
    }
    /// Generated gRPC service name
    pub const SERVICE_NAME: &str = "google.cloud.managedkafka.v1.ManagedKafka";
    impl<T> tonic::server::NamedService for ManagedKafkaServer<T> {
        const NAME: &'static str = SERVICE_NAME;
    }
}
